{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climatology and Anomalies\n",
    "\n",
    "It is very common in climate data analysis to look primarily at `anomalies` or departures from normal, where normal is defined by the `climatology` or seasonal cycle. \n",
    "\n",
    "We typically wish to research and understand other aspects of the climate system than the seasonal cycle which is well understood due to differences in \n",
    "solar radiation associated with the tilt of the Earth's axis. \n",
    "\n",
    "In simple terms, no one is impressed if we can say it will be warm in the summer or cold in the winter or if we can say it will rain in the rainy season and be dry during the dry season.  \n",
    "\n",
    "Therefore, we typically perform our climate data analysis on anomalies by first calculating and removing the climatology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in some data and calculate a climatology...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in monthly temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/pdirmeye/classes/clim680_2022/GHCN_CAMS/'\n",
    "file='air.mon.mean.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(path+file)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(ds.lon,ds.lat,ds['air'][0,:,:],cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the climatology for monthly averaged data\n",
    "\n",
    "The average value of a variable at a given location due to the seasonal cycle.  \n",
    "\n",
    "For *monthly* averaged data we typically calculate the climatology as the average value for a given month over all years.\n",
    "\n",
    "Mathematically, let _T_ be temperature, then\n",
    "_T(i,j)_ is the temperature at some point (_i_,_j_). \n",
    "\n",
    "If we have _N_ years of data, then we can calulate the climatology of the temperature at a point (_i_,_j_) for a given month (_m_) as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{T_m(i,j)} = \\frac{1}{N}\\sum_{k=1}^NT_{m,k}(i,j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a point with data (Washington DC 38.9072° N, 77.0369° W)\n",
    "\n",
    "Notice - since we are naming the one variable `air` when we select the grid cell, we extract an `xarray.DataArray` from the `xarray.Dataset`.\n",
    "We use `da` in the variable name to remind us of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pt = ds['air'].sel(lat=39,lon=360-77,method='nearest')-273.15\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.plot(da_pt.time,da_pt)\n",
    "plt.title('Washington DC, Temperature (˚C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can calculate the climatology using `groupby`\n",
    "\n",
    "We can use `groupby` to group over `time.month` and then apply the mean function to that grouping to get the average value for a given month over our entire grid.\n",
    "\n",
    "This will take a little while - we are crunching through 72 years of data at 0.5˚ resolution. This is **big data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_climo = ds.groupby('time.month').mean(dim='time')\n",
    "ds_climo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at our climatology\n",
    "Plot the climatology along with the data for 1950 and data for 2019 for Washington DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_ptclimo = ds_climo['air'].sel(lat=39,lon=360-77,method='nearest') - 273.15\n",
    "da_pt1950 = da_pt.sel(time=slice('1950-01-01','1950-12-01'))\n",
    "da_pt2019 = da_pt.sel(time=slice('2019-01-01','2019-12-01'))\n",
    "\n",
    "plt.plot(da_ptclimo,label='Climo')\n",
    "plt.plot(da_pt1950,label='1950')\n",
    "plt.plot(da_pt2019,label='2019')                    \n",
    "\n",
    "plt.title('Washington DC, Temperature (K)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anomalies by subtracting the climatology from the original data\n",
    "\n",
    "\\begin{equation}\n",
    "T_{m,k}^{\\prime}(i,j) = T_{m,k}(i,j) - \\overline{T_m(i,j)}\n",
    "\\end{equation}\n",
    "\n",
    "Like when we calculated the climatology, we are crunching through all of this very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_anoms = ds.groupby('time.month')-ds_climo\n",
    "ds_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_anomspt = ds_anoms['air'].sel(lat=39,lon=360-77,method='nearest')\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.plot(da_anomspt['time'],da_anomspt)\n",
    "plt.title('Washington DC, Temperature Anomalies (K)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a long-term running mean to our plot of anomalies so we can see trends more clearly. \n",
    "In fact, let's add two: a 12-month running mean, and a 10-year running mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_smooth_1y = da_anomspt.rolling(time=12).mean()\n",
    "da_smooth_10y = da_anomspt.rolling(time=120).mean()\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.plot(da_anomspt['time'],da_anomspt,label='Monthly',c='plum')\n",
    "plt.plot(da_smooth_1y['time'],da_smooth_1y,label='1y Running Mean',c='teal')\n",
    "plt.plot(da_smooth_10y['time'],da_smooth_10y,label='10y Running Mean',c='red')\n",
    "plt.axhline(y=0,c='white')\n",
    "plt.title('Washington DC, Temperature Anomalies (K)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Colorbars\n",
    "\n",
    "Now that we have anomalies, we often wish to plot with a diverging colorbar centered at zero.\n",
    "\n",
    "#### Plot with off center range and colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clevs = np.arange(-5,11,1)\n",
    "fig = plt.figure(figsize=(11,8.5))\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "cs = ax.contourf(ds_anoms['lon'], ds_anoms['lat'][:-60], \n",
    "               ds_anoms['air'][-1,:-60,:],clevs,\n",
    "               transform = ccrs.PlateCarree(),cmap='RdBu_r',\n",
    "               extend='both')\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "cbar = plt.colorbar(cs,shrink=0.7,orientation='horizontal',\n",
    "                    label='Surface Air Temperature Anomaly (˚C)')\n",
    "plt.title(ds.attrs['title'],fontsize=16)\n",
    "plt.figtext(0.5,0.28,'March 2020',ha='center',fontsize=20,fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center the colorbar at zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,8.5))\n",
    "\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "divnorm = colors.CenteredNorm(vcenter=0)\n",
    "#norm=divnorm\n",
    "cs = ax.contourf(ds_anoms['lon'], ds_anoms['lat'][:-60], \n",
    "                 ds_anoms['air'][-1,:-60,:],clevs,\n",
    "                 transform = ccrs.PlateCarree(),cmap='RdBu_r',\n",
    "                 norm=divnorm,extend='both')\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "cbar = plt.colorbar(cs,shrink=0.7,orientation='horizontal',\n",
    "                    label='Surface Air Temperature (K)')\n",
    "plt.title(ds.attrs['title'],fontsize=16)\n",
    "plt.figtext(0.5,0.28,'March 2020',ha='center',fontsize=20,fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating climatology for daily or higher frequency data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more complicated and more controversial in terms of the method to use.  It also gets us into some issues of handling large datasets depending on how much data there is, which we will talk about in more detail in another class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can start by calculating the average over all years for each day\n",
    "\n",
    "Data: Daily Precipitation from CPC over Continental US (CONUS)\n",
    "https://kpegion.github.io/COLA-DATASETS-CATALOG/precip.V1.0.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_daily = '/home/pdirmeye/classes/clim680_2022/CPC_precip_daily/'\n",
    "files_daily = 'precip.V1.0.*.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_daily = xr.open_mfdataset(path_daily+files_daily,\n",
    "                             concat_dim='time',combine='nested')\n",
    "ds_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_daily_climo = ds_daily.groupby('time.dayofyear').mean()\n",
    "ds_daily_climo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pt = ds_daily['precip'].sel(lat=39,lon=360-97,method='nearest')\n",
    "daily_ptclimo = ds_daily_climo['precip'].sel(lat=39,lon=360-97,method='nearest')\n",
    "daily_pt1948 = daily_pt.sel(time=slice('1948-01-01','1948-12-31'))\n",
    "\n",
    "plt.plot(daily_pt1948)\n",
    "plt.plot(daily_ptclimo)\n",
    "plt.legend(['1948','Climo'])\n",
    "plt.title('CPC Precipitation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This version of climatology will be very noisy. \n",
    "\n",
    "This means if varies a lot from day to day.  Since what we really want in a climatology is to identify the seasonal cycle, which means the wet and dry parts of the year, typically we would smooth this daily climatology in some way or try to identify a cyclical part of the data with a seasonal timescale. Here, I will demonstrate smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_climo_smooth = ds_daily_climo.rolling(dayofyear=30,center=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I get an error that is telling me that my data is too large for the computer to handle smoothing it.  It tells me what I can do to deal with it. It is trying to `chunk` my data into pieces so that it can work on separate parts of it instead of all of it at the same time. It is using something called `dask` behind the scenes to handle my data in parallel.  We will talk more about this next week -- for now I will just do what it says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_daily_climo.chunk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_daily_climo = ds_daily_climo.chunk({'dayofyear':-1})\n",
    "daily_climo_smooth = ds_daily_climo.rolling(dayofyear=30,center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_smoothpt = daily_climo_smooth['precip'].sel(lat=39,lon=360-97,method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily_pt1948)\n",
    "plt.plot(daily_ptclimo)\n",
    "plt.plot(ds_smoothpt)\n",
    "plt.legend(['1948','Climo','Smooth'])\n",
    "plt.title('CPC Precipitation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
