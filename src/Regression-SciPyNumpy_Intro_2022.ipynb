{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "\n",
    "#### What is linear regression?\n",
    "\n",
    "Regression is a method to determine the linear relationship between two variables by determining the best fit line between them.  Using this, we are trying to come up with a model or predictive relationship.\n",
    "\n",
    "\n",
    "Remember that the equation for a line is:\n",
    "\n",
    "$y=mx+b$\n",
    "\n",
    "$m$ = _slope_ (sometimes called the regression coefficient)\n",
    "\n",
    "$b$ = $y$_-intercept_ (value of $y$ when $x=0$)\n",
    "\n",
    "$y$ = _predictand_ or _dependent variable_\n",
    "\n",
    "$x$ = _predictor_ or _independent variable_\n",
    "\n",
    "\n",
    "Linear regression is used to determine the *b* and *m* that defines this best fit line. \"Best fit\" is typically defined as the line that minimizes the _root mean square error_ (RMSE) across all the points between their actual values of $y$ and the values of $y$ on the regression line that share the same values of $x$ as the points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with an idealized example first\n",
    "The `numpy` function [random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) produces a pseudo-random set of values haivng a [_normal_ distribution](https://en.m.wikipedia.org/wiki/Normal_distribution) about the provided _mean_ value, and a spread determined by a provided value of _standard deviation_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(5.0,1.0,200) # Arguments are: (mean, std. deviation, N)\n",
    "m = 3    # Slope\n",
    "b = 60   # Intercept\n",
    "noise = np.random.normal(0,1.0,200) # This is used to add some spread around our line\n",
    "y = m * x + b + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = mx + b$ places all the values of $x$ (in the array `x` we generated above) along a straight line. \n",
    "\n",
    "$y = mx + b + \\eta$ adds a bit of `noise` $\\eta$ to the values of $y$, so they spread away from the straight line somewhat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot([x.min(),x.max()], [m*x.min()+b,m*x.max()+b], c='k', label=\"Without noise\") \n",
    "plt.legend() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem\n",
    "Typically, what we have are two timeseries, $x$ and $y$ and we do not know what $m$ and $b$ are. \n",
    "\n",
    "We use linear regression to determine the _slope_ $m$ and _intercept_ $b$ that best fit a line between the two datasets.\n",
    "\n",
    "Let's pretend we don't know what $m$ and $b$ are in this case. How can we find them from the arrays of $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine slope and y-intercept using `np.polyfit`\n",
    "\n",
    "The `numpy` function [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) takes $x$ and $y$ as input. It also takes as its third argument the degree of the polynomial to fit.  For a linear regression problem, the degree is 1.\n",
    "\n",
    "\n",
    "\n",
    "This function is more powerful than just linear regression. It can fit our data to different polynomials.  \n",
    "\n",
    "$p(x) = p_0 x^0 + p_1 x^1 + p_2 x^2 + p_3 x^3 + ... + p_n x^n$\n",
    "\n",
    "<u>It returns the vector $p$ in reverse order</u>, so \n",
    "the highest degree power is first. \n",
    "\n",
    "Here, we will use it only for fitting a line, so in our case, we have:\n",
    "\n",
    "$p(x) =$ `p[1]` $x^0 +$ `p[0]` $x^1$, where\n",
    "\n",
    "`m = p[0]` (slope)\n",
    "\n",
    "`b = p[1]` (intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[m_fit,b_fit] = np.polyfit(x, y, 1)\n",
    "print(f\"Slope = {m_fit}, Intercept = {b_fit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that this is not perfect. There is some roundoff error due to numerical precision. But it is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the linear regression result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = m_fit * x + b_fit\n",
    "\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot([x.min(),x.max()], [m*x.min()+b,m*x.max()+b], 'k', label=\"Without noise\") \n",
    "plt.plot(x, y_fit, 'r--', label=\"Linear regression\")\n",
    "plt.legend() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency of Variables\n",
    "We typically regress $y$ against $x$ because we usually interpret $y$ to be caused by, \n",
    "or predicted by, the variations in $x$. For this reason, we call:\n",
    "* $x$ is the _independent variable_\n",
    "* $y$ is the _dependent variable_ (i.e., it is dependent on $x$)\n",
    "\n",
    "But often in climate science, cause and effect are not clear cut.\n",
    "Would we get the same result if we regressed $x$ against $y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress X on Y instead of Y on X:\n",
    "[m_tif,b_tif] = np.polyfit(y,x,1)\n",
    "print(f\"Slope = {m_tif}, Intercept = {b_tif}\")\n",
    "x_tif = m_tif * y + b_tif\n",
    "\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot([x.min(),x.max()], [m*x.min()+b,m*x.max()+b], 'k', label=\"Without noise\") \n",
    "plt.plot(x, y_fit, 'r--', label=r\"$y=f(x)$\")\n",
    "plt.plot(x_tif, y, 'g:', label=r\"$x=f(y)$\")\n",
    "plt.legend() ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventionally, the slope and y-intercept are determined as the line that minimizes the squared _vertical_ distance on the plot between the line (red dashed) and each point, summed over all the points.\n",
    "\n",
    "When we regress $x$ on $y$, The slope and <u>x-intercept</u> are determined as the line that minimizes the squared <u>_horizontal_</u> distance between on the plot the line (green dotted) and each point, summed over all the points.\n",
    "\n",
    "### R-Squared\n",
    "\n",
    "The line does not perfectly represent $y$. If there is a strong _linear relationship_, it will represent it well. If not, it may represent it poorly.\n",
    "\n",
    "The part of $y$ not represented by the line, is called the residual. You can calculate it as:\n",
    "\n",
    "$y_{res} = y_{fit}-y$\n",
    "\n",
    "We mentioned RMSE earlier; it is defined as the square root of the average of the squared values of this residual. For $N$ points:\n",
    "\n",
    "RMSE $= \\sqrt{\\frac{1}{N}\\sum{y_{res}^2}}$\n",
    "\n",
    "One way we quantify how well the regression line fits the data is to determine the amount of variation (or variance) in $y$ can be explained by the dependence on $x$ for that regression model. This is called _explained variance_ $R^2$.  More formally, it is:\n",
    "\n",
    "$ R^2=1-\\frac{Unexplained Variance}{Total Variance} $\n",
    "\n",
    "* Unexplained Variance is the variance of the residual $y$\n",
    "\n",
    "* Total Variance is the variance of $y$\n",
    "\n",
    "* You can multiply by 100 and think of it as the percentage variance explained by the regression model\n",
    "\n",
    "A larger $R^2$ indicates a better fit and means that the linear regression model (the best fit line) can explain well the variation of the output with different inputs.\n",
    "\n",
    "...and, yes, the $R$ here is simply _correlation_, which we learend about earlier in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = y_fit - y\n",
    "r_squared = 1 - (np.var(y_res)/np.var(y))\n",
    "print(f\"{100*r_squared:0.2f}% of variance explained by linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can fit a line to anything...\n",
    "...but that doesn't mean we should. The test of explained variance can tell us how well a linear regression is performing as a means to explain the assumption that $y=f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rand = np.random.randn(len(x)) \n",
    "[m_fit_rand,b_fit_rand] = np.polyfit(x,y_rand,1)\n",
    "y_fit_rand = m_fit_rand * x + b_fit_rand\n",
    "print(f\"Slope = {m_fit_rand}, Intercept = {b_fit_rand}\")\n",
    "\n",
    "plt.scatter(x, y_rand, label=\"Data\")\n",
    "plt.plot(x, y_fit_rand, 'r--', label=r\"$y=f(x)$\")\n",
    "plt.legend() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress X on Y instead of Y on X:\n",
    "[m_tif_rand,b_tif_rand] = np.polyfit(y_rand,x,1)\n",
    "print(f\"Slope = {m_tif_rand}, Intercept = {b_tif_rand}\")\n",
    "x_tif_rand = m_tif_rand * y_rand + b_tif_rand\n",
    "\n",
    "plt.scatter(x,y_rand, label=\"Data\")\n",
    "plt.plot(x, y_fit_rand, 'r--', label=r\"$y=f(x)$\")\n",
    "plt.plot(x_tif_rand,y_rand,'g:', label=r\"$x=f(y)$\")\n",
    "plt.legend() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared can give us an idea that this line isn't a very good fit to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_rand = y_rand - y_fit_rand\n",
    "r_squared_rand = 1 - (np.var(y_res_rand)/np.var(y_rand))\n",
    "print(f\"{100*r_squared_rand:0.2f}% of variance explained by linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to perform a significance test on the slope to see if it is statistically different from zero.\n",
    "\n",
    "This is not easily done using `np.polyfit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scipy\n",
    "\n",
    "`scipy.stats.linregress`\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "print(f\"Slope, intercept = {slope:.3g}, {intercept:.3g}\")\n",
    "print(f\"Correlation coefficient = {r_value:.3f}\")\n",
    "print(f\"p-Value = {p_value:.3g}\")\n",
    "print(f\"Standard error = {std_err:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**r_value**: The correlation coefficient between the linear regression fit ($y_{fit}$) and the original data ($y$). Square this to get $r^2$.\n",
    "* Positive values corerspond to a positive slope, negative values to a negative slope.\n",
    "* A value of ±1 means a perfect correlation; RMSE=0\n",
    "* A value of 0 means completely uncorrelated, $y$ is perfectly random with respect to $x$.\n",
    "\n",
    "**p_value**: Two-sided p-value for a hypothesis test whose null hypothesis is that the slope is zero, using Wald Test with t-distribution of the test statistic.\n",
    "* p-value can be interpreted as the likelihood that such a relationship would arise from chance\n",
    "* The smaller the p-value, the less likely the relationship is accidental\n",
    "\n",
    "**std_err**: The absolute measure of the typical distance that the data points fall from the regression line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label=\"Data\")\n",
    "string = (f\"Linear regression\\n\"\n",
    "          f\"($r^2$={r_value**2:0.3f})\")\n",
    "plt.plot(x, intercept + slope*x, 'r-', label=string)\n",
    "plt.legend() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "\n",
    "y = $\\beta$X + $\\epsilon$\n",
    "\n",
    "Where \n",
    "\n",
    "y = predictand/dependent variable\n",
    "\n",
    "X = matrix of the predictor/independent variables, plus vector of one for the intercept term\n",
    "\n",
    "$\\epsilon$ is the error term\n",
    "\n",
    "$\\beta$ is the vector of regression coefficients associated with each independent variable and the intercept\n",
    "\n",
    "The solution for the $\\beta$ is \n",
    "\n",
    "$\\beta$ = $( X^{T} X )^{-1}X^{T}y$\n",
    "\n",
    "Here is a short video describing the geometric least squares solution https://www.youtube.com/watch?v=Z0wELiinNVQ \n",
    "*Note that the notation used in the video is different*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the beginning of this notebook recall that x is a vector of random numbers, m = 3 & b = 60\n",
    "# Let's use this to generate a synthetic dataset with two independent variables\n",
    "x1 = x\n",
    "x2 = np.random.normal(5.0,1.0,200) # (mean, std. deviation, N)\n",
    "m1 = m\n",
    "m2 = 5  # A different slope for x2\n",
    "y = m1 * x1 + m2 * x2 + b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define matrix of independent variables (columns = each independent variable)\n",
    "X = np.zeros((len(x2), 2)) # 2 independent variables x1 and x2\n",
    "X[:,0] = x1\n",
    "X[:,1] = x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(y, X[:,0], X[:,1], marker=m)\n",
    "ax.set_xlabel('Y')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Regression (Ordinary Least Squares Regression) calculation from\n",
    "https://towardsdatascience.com/multiple-linear-regression-from-scratch-in-numpy-36a3e8ac8014 implemented with OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquares(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coefficients = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if len(X.shape) == 1: X = self.__reshape_x(X)\n",
    "            \n",
    "        X = self._concatenate_ones(X)\n",
    "        self.coefficients = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "        \n",
    "    def predict(self, entry):\n",
    "        b0 = self.coefficients[0]\n",
    "        other_betas = self.coefficients[1:]\n",
    "        predictions = b0\n",
    "        \n",
    "        for xi, bi in zip(entry, other_betas): predictions += (bi * xi)\n",
    "        return predictions\n",
    "        \n",
    "    def _reshape_x(self, X):\n",
    "        return X.reshape(-1, 1)\n",
    "    \n",
    "    def _concatenate_ones(self, X):\n",
    "        ones = np.ones(shape=X.shape[0]).reshape(-1, 1)\n",
    "        return np.concatenate((ones, X), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OrdinaryLeastSquares()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "b_est=model.coefficients[0]\n",
    "print(b_est)\n",
    "m1_est=model.coefficients[1]\n",
    "print(m1_est)\n",
    "m2_est=model.coefficients[2]\n",
    "print(m2_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(y, X[:,0], X[:,1], marker=m)\n",
    "\n",
    "y_fit = m1_est * np.array(list(range(3,8,4))) + m2_est * np.array(list(range(3,8,4))) + b_est\n",
    "plt.plot(y_fit, np.array(list(range(3,8,4))), np.array(list(range(3,8,4))),'r')\n",
    "ax.set_xlabel('Y')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
