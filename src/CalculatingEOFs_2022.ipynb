{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Guide to Calculating EOFs\n",
    "\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "Most programming languages have built-in functions to do the covariance, eigenvectors, and eigenvalue calculations for us.  \n",
    "\n",
    "Our first job is to make sure we set up the data to use these functions correctly.  \n",
    "\n",
    "Most of the work to calculate EOFs is in the preparation of our data. \n",
    "\n",
    "* garbage in = garbage out\n",
    "\n",
    "* garbage can look like exciting results\n",
    "\n",
    "Here's what we need to do:\n",
    "\n",
    "* subset to the region of interest (this is an important choice because the result is highly dependent on the region selected; this is where your climate expertise is important)\n",
    "* anomalies \n",
    "* no missing values\n",
    "* weight the data by cosine of latitude\n",
    "* re-shaped to be `[time,space]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in monthly SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/pdirmeye/classes/clim680_2022/OISSTv2/monthly/sst.mnmean.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make anomalies, reverse latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_climo = ds.groupby('time.month').mean()\n",
    "ds_anoms = ds.groupby('time.month')-ds_climo\n",
    "#ds_anoms\n",
    "\n",
    "ds_anoms = ds_anoms.reindex(lat=list(reversed(ds_anoms['lat'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Tropical Pacific Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tpac = ds_anoms.sel(lat=slice(-10,10),lon=slice(105,280))\n",
    "\n",
    "# A quick plot to see what it looks like:\n",
    "fig = plt.figure(figsize=(11,3.5))\n",
    "clevs = [-3,-2,-1.5,-1,-0.5,0,0.5,1,1.5,2,3]\n",
    "plt.contourf(ds_tpac['lon'],ds_tpac['lat'],ds_tpac['sst'][12,:,:],clevs,cmap='coolwarm',extend='both')\n",
    "plt.colorbar(orientation='horizontal',aspect=40,pad=0.20)   # Note how we manipulate the shape and position of the colorbar\n",
    "plt.title(f\"Sample SST Anomaly - {str(ds_tpac['time'][12].values).split('T')[0]}\")\n",
    "plt.xlabel(\"Longitude (Degrees East)\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "ds_tpac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No missing or extraneous values\n",
    "\n",
    "Our data has been \"filled\" over land with interpolated values.  So this is not a problem. \n",
    "\n",
    "What if I had land values that were all missing or other `nan` values?\n",
    "\n",
    "* For values like over land that will be missing throughout all time, you can set them to zero for the EOF calculation. Because they will have no variance, they will not impact the calculation.\n",
    "\n",
    "* For other `nan` values like occasional missing data, you will need to interpolate or otherwise find a way to fill the values with a reasonable value that does not impact the variance. \n",
    "\n",
    "* You may also may need to check for unphysical outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting of the data \n",
    "\n",
    "__Why do we do this?__\n",
    "\n",
    "Atmosphere and ocean data has higher variance in mid-latitudes than in the tropics. Since we are maximizing variance, we are guaranteed that data closer to the equator will have less variance than data closer to the poles. \n",
    "\n",
    "It is common convention to weight our data by the square root of the cosine of the latitude before calculating the EOFs to mitigate this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coslat = np.cos(np.deg2rad(ds_tpac.coords['lat'].values))\n",
    "wgts = np.sqrt(coslat)[..., np.newaxis]\n",
    "ds_tpac = ds_tpac*wgts\n",
    "wgts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape from `[lat,lon,time]` to `[time,space]`\n",
    "\n",
    "We will calculate a 1-dimensional array of eigenvalues (each corresponding to a principal component or PC),\n",
    "and a 2-dimensional array of eigenvectors (one dimension for the PCs, \n",
    "and one for the dimension of our data over which the PCs will vary).\n",
    "\n",
    "The EOF algorithm below needs the first dimension of our matrices to be the one corresponding to the dimension over which \n",
    "the PCs will vary - in our case, that is _time_. So we need to reshape the data.\n",
    "\n",
    "Furthermore, to calculate the covariance matrix in _time_ the data that are not _time_ \n",
    "can only be 1-dimensional, i.e., the matrix on which we calculate covariance with `np.cov` must be 2-dimensional.\n",
    "So we will _ravel_ the latitude and longitude dimensions down to one dimension to do our calculation. \n",
    "We can _unravel_ the resulting EOF spatial patterns back to two dimensions afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = len(ds_tpac['lon'])\n",
    "ny = len(ds_tpac['lat'])\n",
    "nt = len(ds_tpac['time'])\n",
    "\n",
    "X = np.reshape(ds_tpac['sst'].values,((nt,ny*nx)))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Calculation of EOFs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Covariance Matrix\n",
    "The resulting covariance matrix will be square with a size determined by the time dimension.\n",
    "It describes how the spatial pattern of SST in our region in each month co-varies (correlation times variance) with every other month's pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.cov(X)\n",
    "\n",
    "plt.pcolormesh(ds_tpac['time'].values,ds_tpac['time'].values,C,cmap='PRGn_r',vmin=-1.5,vmax=1.5) \n",
    "plt.title(\"Covariance Matrix for Tropical Pacific SSTs\")\n",
    "plt.colorbar(label=\"[$ËšC^2$]\")\n",
    "\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the eigenvalues and vectors of the Covariance Matrix\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_v1,eigenvectors_v1=LA.eig(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation can take awhile for large data sets.  \n",
    "The larger the covariance matrix, the longer it will take.  \n",
    "Finding eigenvalues and eigenvectors is computationally expensive.  \n",
    "There are more efficient ways to do this such as using the singular value \n",
    "decomposition (SVD) function, but this method is easiest to understand.\n",
    "\n",
    "#### Sort eigenvalues and eigenvectors\n",
    "\n",
    "They do not come out of the eig function sorted and we want thme in the order of most variance to least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_v1.shape,eigenvectors_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = eigenvalues_v1.argsort()[::-1]  # Sorts the indices of the matrix based on the magnitudes of the eigenvalues\n",
    "eigenvalues_v1 = eigenvalues_v1[idx]\n",
    "eigenvectors_v1 = eigenvectors_v1[:,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first 10 eigenvalues (note that Python will notate them as array elements 0-9, \n",
    "but it is standard in statistics to label them 1-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,11)\n",
    "plt.plot(x,eigenvalues_v1[0:10]) \n",
    "plt.xticks(x) \n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.xlabel(\"Principal Component\") ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the PC Temporal Patterns\n",
    "\n",
    "They are just the eigenvectors. \n",
    "\n",
    "We saw above that the eigenvector array is square with a size in each dimension equal to the number of time steps.\n",
    "One of these dimensions represents time (the months in our data set),\n",
    "while the other represents the different PCs. \n",
    "\n",
    "There are as many PCs as there are time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_v1=eigenvectors_v1\n",
    "PC_v1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the first couple of PCs (the ones with the most variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc in range(2):\n",
    "    plt.plot(ds_anoms['time'],PC_v1[:,pc],label=f\"PC {pc}\")\n",
    "plt.legend()\n",
    "plt.title(\"Principal Component Time Series for Tropical Pacific SSTs\") ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the EOF Spatial Patterns\n",
    "\n",
    "The spatial pattern is the dot product of the reshaped (`[time,space]`) data and the eigenvector matrix.\n",
    "\n",
    "Remember to \"unweight\" the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_v1=np.dot(X.T,PC_v1)\n",
    "EOF_v1=EOF_v1.reshape((ny,nx,nt)).T/wgts.squeeze()\n",
    "EOF_v1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the first three EOFs (the ones with the most variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols = 3,1\n",
    "clevs = np.arange(-20,21,5)\n",
    "\n",
    "fig,ax = plt.subplots(3,1,layout=\"constrained\",sharex='col')\n",
    "\n",
    "for i in np.arange(3):\n",
    "    panel = ax[i].contourf(ds_tpac['lon'],ds_tpac['lat'],EOF_v1[i,:,:].T,clevs,cmap='RdBu_r',extend='both')\n",
    "    ax[i].set_title(f\"EOF {i+1}\")\n",
    "    \n",
    "fig.colorbar(panel, ax=ax, shrink=0.7) ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the percentage of variance explained by each eigenvector\n",
    "\n",
    "It is the ratio of variance explained by this eigenvector to the total variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vexp = eigenvalues_v1/np.sum(eigenvalues_v1)\n",
    "pct10 = vexp[0:10]*100 # Just the first 10, as percentages\n",
    "\n",
    "# Plot the first 10\n",
    "x = np.arange(1,11)\n",
    "plt.bar(x,pct10)\n",
    "plt.xticks(x) \n",
    "plt.grid(visible=True,which='major',axis='y')\n",
    "plt.ylabel(\"Percent Variance Explained\")\n",
    "plt.xlabel(\"Principal Component\") ;\n",
    "print(pct10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenting EOFs\n",
    "\n",
    "It is common practice to choose a sign convention for our EOFs and multiply the PC timeseries and EOF spatial pattern accordingly.\n",
    "\n",
    "It is also common practice to divide the PC timeseries by its standard deviation and multiply the EOF spatial pattern by the same.  The spatial pattern now has the units of our data (ËšC) and the PC time series is in the units of standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols = 3,1\n",
    "clevs = np.arange(-1.6,1.7,0.2)\n",
    "\n",
    "fig,ax = plt.subplots(3,1,layout=\"constrained\",sharex='col')\n",
    "\n",
    "for i in np.arange(3):\n",
    "    eofnorm = EOF_v1[i,:,:].T*np.std(PC_v1[:,i])\n",
    "    panel = ax[i].contourf(ds_tpac['lon'],ds_tpac['lat'],eofnorm,clevs,cmap='RdBu_r',extend='both')\n",
    "    ax[i].set_title(f\"EOF {i+1}\")\n",
    "    \n",
    "fig.colorbar(panel, ax=ax, shrink=0.7, label=\"ËšC\") ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols = 3,1\n",
    "fig,ax = plt.subplots(nrows,ncols,layout=\"constrained\",sharex='col')\n",
    "\n",
    "for i in np.arange(nrows):\n",
    "    pcnorm = PC_v1[:,i]/np.std(PC_v1[:,i])\n",
    "    panel = ax[i].plot(ds_anoms['time'],pcnorm)\n",
    "    ax[i].set_title(f\"PC {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the other way\n",
    "\n",
    "We could have constructed our matrix along our _raveled_ spatial dimension instead and arrived at (nearly) the same results!\n",
    "\n",
    "#### Calculate the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C=np.cov(X)\n",
    "C=np.matmul(X.T,X)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the eigenvalues and vectors of the Covariance Matrix\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_v2,eigenvectors_v2=LA.eig(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation wii take longer than the one above - now we have a much larger _spatial_ matrix.\n",
    "\n",
    "#### Sort eigenvalues and eigenvectors\n",
    "\n",
    "They do not come out of the eig function sorted and we want the in the order of most variance to least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_v2.shape,eigenvectors_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = eigenvalues_v2.argsort()[::-1]   \n",
    "eigenvalues_v2 = eigenvalues_v2[idx]\n",
    "eigenvectors_v2 = eigenvectors_v2[:,idx]\n",
    "\n",
    "x = np.arange(1,11)\n",
    "plt.plot(x,eigenvalues_v2[0:10]) \n",
    "plt.xticks(x) \n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.xlabel(\"Principal Component\") ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this looks exactly like the set of PCs from version 1 above, except that the values along the Y axis are different. They represent covariances across space rather than time, so are much larger (more numbers summed up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the spatial and temporal patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_v2 = eigenvectors_v2\n",
    "\n",
    "PC_v2 = np.dot(X,EOF_v2) \n",
    "EOF_v2.shape,PC_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the first PC from each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_anoms['time'],PC_v1[:,0]/np.std(PC_v1[:,0]),label=\"Version 1\")\n",
    "plt.plot(ds_anoms['time'],PC_v2[:,0]/np.std(PC_v2[:,0]),label=\"Version 2\") \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops**, did we do something wrong?\n",
    "\n",
    "A: No. The sign of the PC or the EOF is arbirary - it is their product that matters!\n",
    "It is acceptable to multiply a PC by -1, as long as we also multiply its corresponding EOF by -1 so they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's zoom in on the first 2 years\n",
    "plt.plot(ds_anoms['time'][:24],PC_v1[:24,0]/np.std(PC_v1[:24,0]),label=\"Version 1\")\n",
    "plt.plot(ds_anoms['time'][:24],-PC_v2[:24,0]/np.std(PC_v2[:24,0]),label=\"Version 2\") \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the variance explained by each eigenvector\n",
    "\n",
    "It is the ratio of variance explained by this eigenvector to the total variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vexp_v1=eigenvalues_v1/np.sum(eigenvalues_v1)\n",
    "vexp_v2=eigenvalues_v2/np.sum(eigenvalues_v2)\n",
    "pct_v1 = vexp_v1[0:10]*100 # Just the first 10, as percentages\n",
    "pct_v2 = vexp_v2[0:10]*100 # Likewise\n",
    "\n",
    "# Plot the first 10\n",
    "x = np.arange(1,11)\n",
    "plt.bar(x,pct_v1,color=\"#00000000\",edgecolor=\"tab:blue\",linewidth=2,label=\"Approach 1\")\n",
    "plt.bar(x,pct_v2,color=\"#00000000\",edgecolor=\"tab:orange\",width=0.55,linewidth=2,label=\"Approach 2\")\n",
    "plt.xticks(x) \n",
    "plt.legend()\n",
    "plt.grid(visible=True,which='major',axis='y')\n",
    "plt.ylabel(\"Percent Variance Explained\")\n",
    "plt.xlabel(\"Principal Component\") ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenting EOFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_v2=EOF_v2.reshape((ny,nx,ny*nx)).T/wgts.squeeze()\n",
    "\n",
    "nrows,ncols = 3,1\n",
    "clevs = np.arange(-1.6,1.7,0.2)\n",
    "\n",
    "fig,ax = plt.subplots(3,1,layout=\"constrained\",sharex='col')\n",
    "\n",
    "for i in np.arange(3):\n",
    "    eofnorm = EOF_v2[i,:,:].T*np.std(PC_v2[:,i])\n",
    "    panel = ax[i].contourf(ds_tpac['lon'],ds_tpac['lat'],eofnorm,clevs,cmap='RdBu_r',extend='both')\n",
    "    ax[i].set_title(f\"EOF {i+1}\")\n",
    "    \n",
    "fig.colorbar(panel, ax=ax, shrink=0.7, label=\"ËšC\") ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols = 8,1\n",
    "fig,ax = plt.subplots(nrows,ncols,figsize=(5,8),\n",
    "                      layout=\"constrained\",sharex='col')\n",
    "\n",
    "for i in np.arange(int(nrows/2)):\n",
    "    pcnorm = PC_v2[:,i]/np.std(PC_v2[:,i])\n",
    "    panel = ax[i].plot(ds_anoms['time'],pcnorm,color='tab:orange')\n",
    "    ax[i].set_title(f\"PC {i+1}, Approach 2\")\n",
    "    \n",
    "for i in np.arange(int(nrows/2)):\n",
    "    pcnorm = PC_v1[:,i]/np.std(PC_v1[:,i])\n",
    "    panel = ax[i+int(nrows/2)].plot(ds_anoms['time'],pcnorm,color='tab:blue')\n",
    "    ax[i+int(nrows/2)].set_title(f\"PC {i+1}, Approach 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that PC 1 matches, but the sign is arbitrarily reversed (this will happen half the time), and PC 2 matches.\n",
    "However, PC 3 from the first approach matches PC4 from the second approach (and vice versa but again with the sign reversed).\n",
    "What happened?\n",
    "\n",
    "Recall we have sorted our PCs based on their explained variance. It happens that the two different methods gave opposite ranks to the 3rd and 4th PCs. This is quite common, especially for the higher numbered EOFs that have ever decreasing explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
