{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Large Datasets\n",
    "\n",
    "Sometimes in climate, we have very large datasets that don't all fit in memory or even if they do will take a long time to calculate. We can use `dask` to handle the data in `chunks` that will fit in memory and/or be computed in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Arrays\n",
    "\n",
    "[Dask](https://dask.org/)\n",
    "\n",
    "[Dask and Xarray](http://xarray.pydata.org/en/stable/dask.html)\n",
    "\n",
    "A `dask` array looks and feels a lot like a `numpy` array. However, a `dask` array doesn’t directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called “lazy”; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution.\n",
    "\n",
    "From \n",
    "https://earth-env-data-science.github.io/lectures/dask/dask_arrays.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a big array...\n",
    "...as a `numpy` object using the `numpy` method `ones`, which creates an array filled with the number 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=(1000,4000)\n",
    "ones_np=np.ones(shape)\n",
    "ones_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{ones_np.nbytes/1e6} million bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of this array containing 4 million floating point numbers is 32 million bytes.* \n",
    "That means 8 bytes per number.\n",
    "This is \"double precision\", and is the default for floating point numbers in Python.\n",
    "\"Single precision\" would be 4 bytes per number.\n",
    "\n",
    "\\*Note that a \"megabyte\" (MB) is not exactly equal to one million bytes, as a \"kilobyte\" (KB) is not exactly 1000 bytes. \n",
    "Rather, 1 KB = 1024 bytes, and 1 MB = 1,048,576 bytes.\n",
    "\n",
    "### Dask objects\n",
    "\n",
    "We can also create a big array like this as a `dask` object using the corresponding `dask` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones=da.ones(shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see looks just like what have seen for DataArrays in `xarray`. \n",
    "This is because `xarray` uses `dask` for its data management.\n",
    "\n",
    "### Chunks\n",
    "\n",
    "Notice that we have two columns in our table: \n",
    "1. The first is called \"Array\", and its meaning should be obvious.\n",
    "2. The second is called \"Chunk\". It describes how the data are grouped in the computer's memory.\n",
    "\n",
    "In this case, there is only one \"chunk\" containing all the data in the array.\n",
    "However, we can specify that this large array should be broken into smaller pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape=(1000,1000)\n",
    "ones=da.ones(shape,chunks=chunk_shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `dask` depiction of the data is now different. \n",
    "We now see that there are 4 chunks, each with the square shape we specified (1000x1000).\n",
    "Each has a size in bytes that is 1/4 the size of the entire array.\n",
    "Also, the pictoral representation shows this division into chunks with vertical lines.\n",
    "\n",
    "Try some other values for chunk shapes... see what happens.\n",
    "\n",
    "What happens if you choose a chunk dimension that does not divide evenly into the array's dimensions?\n",
    "\n",
    "### Dask methods\n",
    "\n",
    "`dask` is _lazy_. That is, `dask` will not initiate a calculation when it encounters a line of code _unless_ its results are to be:\n",
    "1. displayed (as either text output or a graphical plot)\n",
    "2. written to disk\n",
    "3. shared with another non-dask function that needs the result for its operation\n",
    "\n",
    "This is done to conserve memory, and speed the execution of the code. Unlike the example above, most of the time\n",
    "`dask` is employed when reading data from files stored on disk, or in the cloud. \n",
    "Often these are large files, and/or many files, that are opened at once. \n",
    "Performing calculations with large datasets on disk is an operation that is **IO bound**, meaning that the\n",
    "speed limitations of input and output (hard disk speeds, networking, etc) are the main factor that\n",
    "slows computations. `dask` is designed to avoid unnecessary input/output (IO) as well as conserve \n",
    "computer memory.\n",
    "\n",
    "The combination of the _laziness_ of `dask` and its chunking feature is often used in distributed computing environments like\n",
    "clusters. It can allow a computation to spread across multiple CPUs using parallel computing, much like MPI or Open MP in \n",
    "computing languages like FORTRAN.\n",
    "\n",
    "One of the methods to spur `dask` calculations into action is thorugh one of several special `dask` commands or methods.\n",
    "One is `compute`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ones.compute()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our array called `ones` did not actually exist until the previous cell was executed. \n",
    "What we saw above were descriptions of what `ones` _would look like_, how much memory it _would require_ and \n",
    "how the chunks _would be distributed_ once the array was in memory. \n",
    "But memory space was not occupied until we executed the command:\n",
    "`test = ones.compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "--------------------\n",
    "\n",
    "### Visualizing what Dask does\n",
    "\n",
    "There is a handy method called `visualize` that shows how `dask` defines chunks and manages them as it performs calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ones.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones=ones.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation=(ones*ones[::-1,::-1]).mean()\n",
    "fancy_calculation.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "------------------\n",
    "### A Big Calculation\n",
    "\n",
    "The examples above were toy examples (32Mb).  This data is not big enough to warrant the use of `dask`.  Let's try a much bigger example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigshape=(200000,4000)\n",
    "big_ones=da.ones(bigshape,chunks=chunk_shape)\n",
    "big_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_ones.nbytes/1e9,\"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT VISUALIZE THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask has some other tools to help us understand what is happening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "big_calc = (big_ones * big_ones[::-1, ::-1]).mean()\n",
    "\n",
    "with ProgressBar():\n",
    "    result = big_calc.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the usual `numpy` (and `xarray`) methods work on `dask` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ones_reduce=(np.cos(big_ones)**2).mean(axis=0)\n",
    "big_ones_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Xarray uses dask by default when you use open_mfdataset \n",
    "`dask` can be invoked by specifying `chunks` when you open and read your data.\n",
    "\n",
    "Example: ERA5 daily atmospheric data (multiple presure levels)\n",
    "File for every day from 1979-2020 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/pdirmeye/ERA5_z/'\n",
    "fname = 'ea_global_an_daily_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of many files to open and read\n",
    "fnames = path+fname+'*.nc4'\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - you can use a wildcard string in the file name to open multiple files\n",
    "ds=xr.open_mfdataset(fnames,combine='nested',concat_dim='time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Gaussian grids are a type of irregular grid\n",
    "* [How they work](https://confluence.ecmwf.int/display/FCST/Gaussian+grids)\n",
    "* [The N320 (grid used by ERA5) table by latitude rows](https://confluence.ecmwf.int/display/EMOS/N320)\n",
    "\n",
    "We need to map each of the grid cells in the reduced Gaussian grid onto their corresponding longitudes and latitudes. \n",
    "There is more than one way to do this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to reindex the \"rgrid\" reduced Gaussian grid vector into latitudes and longitudes\n",
    "rgg_file = path+'N320_reduced_grid.nc4'\n",
    "rgg = xr.open_dataset(rgg_file)\n",
    "# rgg is an rgrid-length pair of vectors for the corresponding latitudes and longitudes from the rectangular grid\n",
    "rgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to reindex the \"rgrid\" reduced Gaussian grid vector into latitudes and longitudes\n",
    "latlon_file = path+'N320_index.nc' \n",
    "latlon = xr.open_dataset(latlon_file)\n",
    "# latlon is 640x1280 lat-lon grid containing the representative \"rgrid\" value to map to each point on the regular grid. \n",
    "latlon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)projecting data\n",
    "\n",
    "* How would the two different reindexing datasets be applied? \n",
    "* What be the result of each?\n",
    "* Could you produce the same grids of data, and the same maps, from them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity = 9.8 # m/s**2\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "ax=plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "plt.scatter(rgg['lon'],rgg['lat'],c=ds[\"z\"][0,0]/gravity,s=0.1,marker='s',transform=ccrs.PlateCarree(),cmap=\"GnBu_r\")\n",
    "ax.coastlines()\n",
    "\n",
    "plt.title('300hPa Geopotential Heights',fontsize=20)\n",
    "plt.colorbar(shrink=0.7,aspect=30,orientation='horizontal',label='meters') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to reconstitute reduced to full Gaussian Grids\n",
    "`era5_remap` is a Python function that remaps reduced grid data onto the full rectangular (1280x640) Gaussian grid.\n",
    "* Functions in a `.py` script file can be imported like any other Python pachage or library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/pdirmeye/classes/clim680_2022/era5_remap.py .\n",
    "from era5_remap import era5_remap\n",
    "help(era5_remap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grid = era5_remap(ds[\"z\"][0,0]/gravity,'rgg',latlon)\n",
    "\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "plt.pcolormesh(reg_grid.lon,reg_grid.lat,reg_grid,cmap='GnBu_r',transform=ccrs.PlateCarree(),shading='nearest')\n",
    "ax.coastlines()\n",
    "\n",
    "plt.title('300hPa Geopotential Heights',fontsize=20)\n",
    "plt.colorbar(shrink=0.7,aspect=30,orientation='horizontal',label='meters') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How long does it take?\n",
    "\n",
    "We can use timers to see how long it takes for blocks of code to run. This is a great way to find inefficiencies and understand code performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "reg_grid = era5_remap(ds[\"z\"][0,0],'rgg',latlon) # One level on one day\n",
    "end_1d = perf_counter()\n",
    "print(f\"Horizontal field required {end_1d-start:.3g} seconds\")\n",
    "\n",
    "reg_grid = era5_remap(ds[\"z\"][0],'rgg',latlon) # All levels on one day\n",
    "end_2d = perf_counter()\n",
    "print(f\"Horizontal and vertical field required {end_2d-end_1d:.3g} seconds\")\n",
    "\n",
    "reg_grid = era5_remap(ds[\"z\"][:32,0],'rgg',latlon) # All levels for all days in the first month\n",
    "end_mo = perf_counter()\n",
    "print(f\"Horizontal and time (one month) field required {end_mo-end_2d:.3g} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there is a signficant amount of _scaling_ here. \n",
    "\"Scaling\" means that the amount of time it takes to do a larger task does not grow as quickly as the task grows.\n",
    "* Processing 5 levels did not take 5x longer than processing 1 level. Less than 2x.\n",
    "* Processing 31 days took around 4 times longer than processing one day.\n",
    "\n",
    "The function `era5_remap` uses a method called [_list comprehension_](https://en.wikipedia.org/wiki/Comparison_of_programming_languages_(list_comprehension)#Python) to regrid the data.\n",
    "* _List comprehension_ is much faster than loops.\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's convert these awkward timestamps into `dtype=datetime64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtobj = pd.to_datetime(list(ds['time'].values), format='%Y%m%d.%f')\n",
    "ds['time'] = dtobj\n",
    "ds['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use our beloved `.groupby()` method to parse through the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = perf_counter()\n",
    "z_climo_jan = ds['z'].groupby('time.month')[1].mean(dim='time')\n",
    "t1 = perf_counter()\n",
    "print(f\"{t1-t0:.3g} seconds\")\n",
    "\n",
    "z_climo_jan.load()\n",
    "t2 = perf_counter()\n",
    "print(f\"{t2-t1:.3g} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see that our big calculation didn't actually happen at the line where we assigned `z_climo_jan`.\n",
    "\n",
    "`dask` does not perform actual calculations or even load the data from a file into memory _until it absolutely must_ (e.g., to plot a result).\n",
    "The `.load()` method forces `dask` to read the data from the file on disk into memory, much as `.compute()` forces `dask` to perform a calculation.\n",
    "\n",
    "#### When would you want to force `dask` to be _eager_ rather than _lazy_?\n",
    "\n",
    "* When you want to use computations over and over\n",
    "    * Example: If you calculate anomalies for a really large dataset and then you want to use the anomalies for the rest of the program without asking `dask` to recompute them each time.\n",
    "<br><br>\n",
    "\n",
    "* When you have a performance issue\n",
    "    * If it is taking a long time to do the calculations, you can tell `dask` to go ahead and `load` the data ahead of time, if you have enough memory, or go ahead and `compute` the computations up to this point.  \n",
    "\n",
    "\n",
    "See how long it is taking with the progress bar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_decade_climo_jul = ds[\"z\"].sel(plev=50000.,time=slice(\"2001-01-01\", \"2010-12-31\")).groupby('time.month')[12].mean(dim='time')\n",
    "\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "with ProgressBar():\n",
    "    era5_remap(z_decade_climo_jul,'rgg',latlon).plot(cmap='GnBu_r',transform=ccrs.PlateCarree()) \n",
    "    \n",
    "ax.coastlines()\n",
    "plt.title('300hPa Heights - Dec (2001-2010)') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not the remapping or the plotting that takes all the time - it is crunching through the calculation across all that data on disk.\n",
    "with ProgressBar():\n",
    "    ds_load=z_decade_climo_jul.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProgressBar` only monitors `dask` actions. Below we are not invoking `dask`, so the function does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "with ProgressBar():\n",
    "    era5_remap(ds_load,'rgg',latlon).plot(cmap='GnBu_r')\n",
    "ax.coastlines()\n",
    "plt.title('300hPa Heights - Dec (2001-2010)') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
