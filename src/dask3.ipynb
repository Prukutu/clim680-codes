{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Large Datasets\n",
    "\n",
    "Sometimes in climate, we have very large datasets that don't all fit in memory or even if they do will take a long time to calculate. We can use `dask` to handle the data in `chunks` that will fit in memory and/or be computed in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Arrays\n",
    "\n",
    "[Dask](https://dask.org/)\n",
    "\n",
    "[Dask and Xarray](http://xarray.pydata.org/en/stable/dask.html)\n",
    "\n",
    "A `dask` array looks and feels a lot like a `numpy` array. However, a `dask` array doesn’t directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called “lazy”; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution.\n",
    "\n",
    "From \n",
    "https://earth-env-data-science.github.io/lectures/dask/dask_arrays.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install graphviz python-graphviz # Will use this later to visualize how dask manages large data calculations\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a big array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=(1000,4000)\n",
    "ones_np=np.ones(shape)\n",
    "ones_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ones_np.nbytes/1e6,\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "ones=da.ones(shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape=(1000,1000)\n",
    "ones=da.ones(shape,chunks=chunk_shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones=ones.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation=(ones*ones[::-1,::-1]).mean()\n",
    "fancy_calculation.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Big Calculation\n",
    "\n",
    "The examples above were toy examples (32Mb).  This data is not big enough to warrant the use of `dask`.  Let's try a much bigger example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigshape=(200000,4000)\n",
    "big_ones=da.ones(bigshape,chunks=chunk_shape)\n",
    "big_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_ones.nbytes/1e9,\"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT VISUALIZE THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask has some tools to help us undertand what is happening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "big_calc = (big_ones * big_ones[::-1, ::-1]).mean()\n",
    "\n",
    "with ProgressBar():\n",
    "    result = big_calc.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the usual `numpy` (and `xarray`) methods work on `dask` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ones_reduce=(np.cos(big_ones)**2).mean(axis=0)\n",
    "big_ones_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Xarray uses dask by default when you use open_mfdataset \n",
    "`dask` can be invoked by specifying `chunks` when you open and read your data.\n",
    "\n",
    "Example: ERA5 daily atmospheric data (multiple presure levels)\n",
    "File for every day from 1979-2020 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path='/shared/working/rean/era-interim/daily/data/'\n",
    "### fname='ei.oper.an.pl.regn128cm.'\n",
    "path = '/shared/land/ERA5/daily/global_3d/z/'\n",
    "fname = 'ea_global_an_daily_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of many files to open and read\n",
    "fnames = path+fname+'*.nc4'\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notie - you can use a wildcard string in the file name to open multiple files\n",
    "ds=xr.open_mfdataset(fnames,combine='nested',concat_dim='time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Gaussian grids are a type of irregular grid\n",
    "* [How they work](https://confluence.ecmwf.int/display/FCST/Gaussian+grids)\n",
    "* [The N320 (grid used by ERA5) table by latitude rows](https://confluence.ecmwf.int/display/EMOS/N320)\n",
    "\n",
    "We need to map each of the grid cells in the reduced Gaussian grid onto their corresponding longitudes and latitudes. \n",
    "There is more than one way to do this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to reindex the \"rgrid\" reduced Gaussian grid vector into latitudes and longitudes\n",
    "rgg_file = '/shared/land/ERA5/N320_reduced_grid.nc4'\n",
    "rgg = xr.open_dataset(rgg_file)\n",
    "# rgg is an rgrid-length pair of vectors for the corresponding latitudes and longitudes from the rectangular grid\n",
    "rgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to reindex the \"rgrid\" reduced Gaussian grid vector into latitudes and longitudes\n",
    "latlon_file = '/shared/land/ERA5/N320_index.nc' \n",
    "latlon = xr.open_dataset(latlon_file)\n",
    "# latlon is 640x1280 lat-lon grid containing the representative \"rgrid\" value to map to each point on the regular grid. \n",
    "latlon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)projecting data\n",
    "\n",
    "* How would the two different reindexing datasets be applied? \n",
    "* What be the result of each?\n",
    "* Could you produce the same grids of data, and the same maps, from them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity = 9.8 # m/s**2\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "plt.scatter(rgg['lon'],rgg['lat'],c=ds[\"z\"][0,0]/gravity,s=1.0,marker='s',cmap=\"GnBu_r\")\n",
    "\n",
    "plt.title('300hPa Heights',fontsize=20)\n",
    "plt.colorbar(label='meters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,8))\n",
    "plt.hexbin(rgg['lon'],rgg['lat'],ds[\"z\"][0,0]/gravity,cmap=\"GnBu_r\")\n",
    "plt.title('300hPa Heights',fontsize=20)\n",
    "plt.colorbar(label='meters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to reconstitute reduced to full Gaussian Grids\n",
    "`era5_remap` is a Python function that remaps reduced grid data onto the full rectangular (1280x640) Gaussian grid.\n",
    "* Functions in a `.py` script file can be imported like any other Python pachage or library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /shared/land/ERA5/era5_remap.py .\n",
    "from era5_remap import era5_remap\n",
    "help(era5_remap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grid = era5_remap(ds[\"z\"][0,0],'rgg')\n",
    "\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "plt.pcolormesh(reg_grid.lon,reg_grid.lat,reg_grid,cmap='GnBu_r',shading='nearest')\n",
    "plt.title('300hPa Heights',fontsize=20)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How long does it take?\n",
    "\n",
    "We can use timers to see how long it takes for blocks of code to run. This is a great way to find inefficiencies and understand code performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "reg_grid = era5_remap(ds[\"z\"][0,0],'rgg') # One level on one day\n",
    "end_1d = perf_counter()\n",
    "print(\"Horizontal field required\",end_1d-start,\"seconds\")\n",
    "\n",
    "reg_grid = era5_remap(ds[\"z\"][0],'rgg') # All levels on one day\n",
    "end_2d = perf_counter()\n",
    "print(\"Horizontal and vertical field required\",end_2d-end_1d,\"seconds\")\n",
    "\n",
    "reg_grid = era5_remap(ds[\"z\"][:32,0],'rgg') # All levels for all days in the first month\n",
    "end_mo = perf_counter()\n",
    "print(\"Horizontal and time (one month) field required\",end_mo-end_2d,\"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there is not a great deal of scaling here. \n",
    "* Processing 31 days takes about 31 times longer than processing one day.\n",
    "* This is because the function `perf_counter` uses a method called _list comprehension_ to regrid the data.\n",
    "* _List comprehension_ is much faster than loops, but it is not parallelizable, unlike `dask`.\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are a few situations when you need to pay attention to the fact that you are using `dask` arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When you get a `chunking` error__\n",
    "\n",
    "Remember when we did `rolling` and got an error\n",
    "\n",
    "`daily_climo_smooth=ds_daily_climo.rolling(dayofyear=30,center=True).mean()`\n",
    "\n",
    "`For window size 30, every chunk should be larger than 15, but the smallest chunk size is 1. Rechunk your array with a larger chunk size or a chunk size that more evenly divides the shape of your array.`\n",
    "\n",
    "`Dask` was telling us that it couldn't work with the data in its current `chunks`, so we `rechunked` it using:\n",
    "`ds_daily_climo=ds_daily_climo.chunk({'dayofyear':-1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert these awkward timestamps into dtype=datetime64 \n",
    "import pandas as pd\n",
    "dtobj = pd.to_datetime(list(ds['time'].values), format='%Y%m%d.%f')\n",
    "ds['time'] = dtobj\n",
    "ds['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use our .groupby() method to parse through the time dimension\n",
    "start = perf_counter()\n",
    "z_climo_jan = ds['z'].groupby('time.month')[1].mean(dim='time')\n",
    "t1 = perf_counter()\n",
    "print(t1-start,\"seconds\")\n",
    "z_climo_jan.load()\n",
    "t2 = perf_counter()\n",
    "print(t2-t1,\"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see that our big calculation didn't actually happen at the line where we assigned `z_climo_jan`.\n",
    "\n",
    "`dask` does not perform actual calculations or even load the data from a file into memory _until it absolutely must_ (e.g., to plot a result).\n",
    "The `.load()` method forces `dask` to read the data from the file on disk into memory, much as `.compute()` forces `dask` to perform a calculation.\n",
    "\n",
    "#### When would you want to force `dask` to be _eager_ rather than _lazy_?\n",
    "\n",
    "* When you want to use computations over and over\n",
    "    * Example: If you calculate anomalies for a really large dataset and then you want to use the anomalies for the rest of the program without asking `dask` to recompute them each time.\n",
    "\n",
    "\n",
    "* When you have a performance issue\n",
    "    * If it is taking a long time to do the calculations, you can tell `dask` to go ahead and `load` the data ahead of time, if you have enough memory, or go ahead and `compute` the computations up to this point.  \n",
    "\n",
    "\n",
    "See how long it is taking with the progress bar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_remap(z_decade_climo_jul,'rgg').plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_decade_climo_jul = ds[\"z\"].sel(plev=50000.,time=slice(\"2001-01-01\", \"2010-12-31\")).groupby('time.month')[12].mean(dim='time')\n",
    "\n",
    "with ProgressBar():\n",
    "    era5_remap(z_decade_climo_jul,'rgg').plot(cmap='GnBu_r') ; plt.title('300hPa Heights - Dec (2001-2010)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not the remapping or the plotting that takes all the time - it is crunching through the calculation across all that data on disk.\n",
    "with ProgressBar():\n",
    "    ds_load=z_decade_climo_jul.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_remap(ds_load,'rgg').plot(cmap='GnBu_r')\n",
    "plt.title('300hPa Heights - Dec (2001-2010)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clim680)",
   "language": "python",
   "name": "clim680"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
