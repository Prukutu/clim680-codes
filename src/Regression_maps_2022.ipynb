{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Maps\n",
    "\n",
    "Often, in climate analysis:\n",
    "\n",
    "* We perform linear regression between an index and the time series of anomalies at each gridpoint.  \n",
    "\n",
    "* Because we use anomalies, *b* is zero (or very nearly so), so the slope is all we need.\n",
    "\n",
    "* We make maps of the slope (also called regression coefficient) as a measure of the direction and strength of the linear relationship.  \n",
    "\n",
    "* We then stipple or mask where this relationship is significant.   \n",
    "\n",
    "We will continue with our previous example of looking at the relationship between the Nino3.4 index and precipitation anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from cartopy.util import add_cyclic_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nino34='/home/pdirmeye/classes/clim680_2022/nino34_1982-2019.oisstv2_anoms.nc'\n",
    "ds_nino34=xr.open_dataset(file_nino34)\n",
    "ds_nino34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/home/pdirmeye/classes/clim680_2022/GPCP_precip.mon.mean.nc'\n",
    "ds_precip=xr.open_dataset(file)\n",
    "ds_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_precip = ds_precip.precip.sel(time=slice(ds_nino34['time'][0],ds_nino34['time'][-1]))\n",
    "\n",
    "da_climo = da_precip.groupby('time.month').mean()\n",
    "da_anoms = da_precip.groupby('time.month')-da_climo\n",
    "da_anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Again, we use the `linregress` function from the `stats` package in `scipy`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop through all the grid cells across the globe and calculate the regression parameters and statistics at each grid cell.\n",
    "Doing this, we populate new global grids with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the size of the global grid array\n",
    "nx = len(da_anoms['lon'])\n",
    "ny = len(da_anoms['lat'])\n",
    "\n",
    "# Create new empty arrays to contain the results of our calculations\n",
    "p_array = np.zeros((ny,nx))   # The p-value (significance) of the fit of the regression\n",
    "r_array = np.zeros((ny,nx))   # The correlation between the independent and dependent variables\n",
    "m_array = np.zeros((ny,nx))   # The slope of the best-fit linear regression line\n",
    "\n",
    "x =       ds_nino34['sst']    # The independent variable (predictor)\n",
    "\n",
    "#### Loop through all the grid cells in the global grid\n",
    "for j in range(ny):\n",
    "    print(f\"{j},\",end=\" \")\n",
    "    for i in range(nx):\n",
    "        \n",
    "        y = da_anoms[:,j,i] # The dependent variable (predictand)\n",
    "        \n",
    "        m,b,r,p,e = linregress(x,y)   # b is intercept, e is standard error\n",
    "        \n",
    "        # Populate our new arrays with the results\n",
    "        m_array[j,i] = m\n",
    "        r_array[j,i] = r\n",
    "        p_array[j,i] = p\n",
    "print(\"*** DONE ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a mask \n",
    "\n",
    "Only include points where our regression coefficient is significantly different from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sig = np.where(p_array<0.05,m_array,np.nan) # NaNs where the signficance test fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the regression coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clevs=np.arange(-3,3.25,0.25)\n",
    "\n",
    "fig = plt.figure(figsize=(11,8.5))\n",
    "\n",
    "# Set the axes using the specified map projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=200))\n",
    "\n",
    "# Add cyclic point\n",
    "data = m_array\n",
    "data,lon = add_cyclic_point(data,coord=da_anoms['lon'])\n",
    "mask_data,lons = add_cyclic_point(mask_sig,coord=da_anoms['lon'])\n",
    "\n",
    "# Make a filled contour plot\n",
    "cs = ax.contourf(lon,da_anoms['lat'],\n",
    "            data,clevs,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap='coolwarm',extend='both')\n",
    "\n",
    "ax.contourf(lon,da_anoms['lat'],mask_data,[0,1],\n",
    "            transform = ccrs.PlateCarree(),colors='None',\n",
    "            hatches=['..','..'],extend='both',alpha=0)\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines()\n",
    "\n",
    "# Add gridlines\n",
    "ax.gridlines()\n",
    "\n",
    "# Define the xticks for longtitude \n",
    "ax.set_xticks(np.arange(-180,181,60),crs=ccrs.PlateCarree())\n",
    "lon_formatter = cticker.LongitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "\n",
    "# Define ytick for latitude\n",
    "ax.set_yticks(np.arange(-90,91,30),crs=ccrs.PlateCarree())\n",
    "lat_formatter = cticker.LatitudeFormatter()\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "# Call colorbar\n",
    "cbar = plt.colorbar(cs,orientation='horizontal',shrink=0.7,\n",
    "                 label='Regression Coefficient (mm/day)')\n",
    "\n",
    "# Add title\n",
    "plt.title('Regression between Nino3.4 and Precipitation Anomalies') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note on Regression\n",
    "\n",
    "The correlation coefficient and regression coefficient tell us similar information about the linear relationship between our two datasets.  \n",
    "\n",
    "The benefit of a linear regression is that we get a measure of that relationship in the form of the regression coefficient. The regression coefficient is the slope of line fit to the data - is is a measure of the _sensitivity_ of $y$ to variations in $x$.\n",
    "\n",
    "The slope (and intercept) of the line also provide us a linear model of this relationship.  If we have a _good_ model, then we can predict the value of $y$ based on new values of $x$. What consitutes a _good_ model and when, how I should use a linear regression as a prediction model, and lots of other details is left to a statistics class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime_as_string(da_anoms['time'][0]).split('T')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "## Writing data to a .nc file - Understanding `xarray` DataArrays and Datasets\n",
    "\n",
    "When you work data sets and perform analysis, in this course and beyond, \n",
    "you will want to preserve your results by writing the data you produce to new files on the computers you are using. \n",
    "The most common format for climate data files is NetCDF, which we will examine here, \n",
    "but these principles apply to any data format, particularly self-describing formats (including HDF and GRIB).\n",
    "\n",
    "### Creating `xarray` DataArrays and Datasets\n",
    "\n",
    "Suppose we want to write out our regression calculation to a file (or apply some xarray function to our data).  If we have a `xarray.Dataset` called `ds`, we could do the following:\n",
    "\n",
    "`ds.to_netcdf('regression.nc')`\n",
    "\n",
    "But `m_array` is not an `xarray` dataset; now it's just a `numpy` array.  How do we create an `xarray` dataset?\n",
    "1. Create `xarray.DataArray` (note the odd capitalization) with named dimensions and assigned coordinates (plus any other attributes we want to add)\n",
    "2. Convert to a `xarray.Dataset`:\n",
    "    * Use the `to_dataset` method to convert a single DataArray into a Dataset.\n",
    "    * Invoke the `xarray.Dataset` object to assign multiple DataArrays to a single Dataset\n",
    "    * Attributes can be assigned to the Dataset that are different from the DataArray attributes\n",
    "    \n",
    "Attributes are to data in self-describing formats (like NetCDF) as comments are to your code. \n",
    "\n",
    "It is good practice and a service to others (and to your future self) to include descriptive and complete documentation in the attributes of your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataArray with all the details\n",
    "da_m = xr.DataArray(m_array,\n",
    "                    coords={'lat':da_anoms['lat'],\n",
    "                            'lon': da_anoms['lon']},\n",
    "                    dims=['lat','lon'],\n",
    "                    name='slope',\n",
    "                    attrs={'name':'regression coefficient',\n",
    "                           'units':'mm/day',\n",
    "                           'description':'Linear regression of monthly precipitation against Niño3.4 index'})\n",
    "# Convert the DataArray into a Dataset\n",
    "ds_m = da_m.to_dataset()\n",
    "ds_m = ds_m.assign_attrs({'start date':np.datetime_as_string(da_anoms['time'][0]).split('T')[0],\n",
    "                          'end date':np.datetime_as_string(da_anoms['time'][-1]).split('T')[0]})\n",
    "ds_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the attributes of our coordinate variables have been inherited from the precipitation data they came from.\n",
    "\n",
    "Since our new variable, which we named `slope` in the `xarray` DataSet, was created from a `numpy` array,\n",
    "it had no attributes to inherit. We had to specify them.\n",
    "\n",
    "For the dataset itself, we added information about the range of time across which the regression was calculated. \n",
    "We took that information from the time dimension of the precipitaiton data.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### Merging multiple variables into a DataSet\n",
    "\n",
    "Suppose we wanted to put m, r, and p together as separate variables in a `xarray.Dataset`.  We can repeat this for all of them (m,r,p):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more DataArrays\n",
    "da_r = xr.DataArray(r_array,\n",
    "                    coords={'lat':da_anoms['lat'],\n",
    "                            'lon': da_anoms['lon']},\n",
    "                    dims=['lat','lon'],\n",
    "                    name='corr',\n",
    "                    attrs={'name':'correlation coefficient',\n",
    "                           'units':'none',\n",
    "                           'description':'Correlation between monthly precipitation and Niño3.4 index'})\n",
    "da_p = xr.DataArray(p_array,\n",
    "                    coords={'lat':da_anoms['lat'],\n",
    "                            'lon': da_anoms['lon']},\n",
    "                    dims=['lat','lon'],\n",
    "                    name='p',\n",
    "                    attrs={'name':'p-value',\n",
    "                           'units':'none',\n",
    "                           'description':'Signficance of regression of monthly precipitation against Niño3.4 index'})\n",
    "\n",
    "# Convert the DataArrays into Datasets\n",
    "ds_r = da_r.to_dataset()\n",
    "ds_p = da_p.to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then merge the DataSets together. \n",
    "Note that each must have different variable names or `xarray` doesn't know how to deal with them.\n",
    "\n",
    "#### Method 1: Merge DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regr = xr.merge([ds_m,ds_p,ds_r])\n",
    "ds_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Add DataArrays to an existing DataSet\n",
    "\n",
    "We had already made a Dataset containing the DataArray for the **slope**. \n",
    "We could just add the two other DataArrays to that Dataset. \n",
    "This is especially handy if you want to add a small number of variables to an exisiting Dataset having many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regr = ds_m\n",
    "ds_regr['p'] = da_p\n",
    "ds_regr['corr'] = da_r\n",
    "ds_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Create Dataset from list of DataArrays\n",
    "\n",
    "Actually, we did not need to make the intermediate Dataset at all - we could have made the Dataset directly from the set of DataArrays.\n",
    "\n",
    "Notice that we also did not need to _name_ the variables in the DataArrays either - we do it at this step.\n",
    "\n",
    "All three methods created the same final result. \n",
    "For a small Dataset like this, there is little difference between these methods.\n",
    "However, depending on your situation, one of these methods may be clearly better (or worse) than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regr = xr.Dataset(data_vars={'slope':da_m,'p':da_p,'corr':da_r})\n",
    "ds_regr = ds_regr.assign_attrs({'start date':np.datetime_as_string(da_anoms['time'][0]).split('T')[0],\n",
    "                                'end date':np.datetime_as_string(da_anoms['time'][-1]).split('T')[0]})\n",
    "ds_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Dataset to a file on disk\n",
    "\n",
    "Now we can write our Dataset containing multiple variables to a single NetCDF file.\n",
    "\n",
    "It is `xarray`'s seamless handling of NetCDF and other self-describing data formats that makes it such a handy Python package for Climate Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regr.to_netcdf('regression.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_check=xr.open_dataset('regression.nc')\n",
    "ds_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing file size: NetCDF _deflation_\n",
    "\n",
    "NetCDF is a very useful file format because it is _self-describing_ (i.e., metadata is stored with the data in the same files).\n",
    "\n",
    "However, versions since Version 4 have an additional useful option that you should use when storing large files. \n",
    "That is _deflation_, which is a file compression feature.\n",
    "It works in the same way that images (`JPEG`) and video (`MPEG`) are compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deflate = dict(zlib=True, complevel=1)          # For deflated NetCDF4 output without data loss\n",
    "encoding = {var: deflate for var in ds_regr}    # Apply to every variable in the Dataset\n",
    "ds_regr.to_netcdf(path='regression.nc4',engine=\"netcdf4\",format=\"netCDF4\",encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l reg*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That isn't much of a difference in this case (less than 8% compression), \n",
    "but for very large files, files with many dimensions,\n",
    "or DataArrays that contain a lot of grid cells with the same value \n",
    "(e.g., zeros or NaNs), the compression rate can be very large. \n",
    "This can save a large amount of disk space, yet the extra time to read\n",
    "deflated files is usually small.\n",
    "\n",
    "#### Some other points to note above:\n",
    "* We had to specify the `engine` and `format` to invoke version 4 of NetCDF.\n",
    "By default, `xarray` uses version 3, which does not support `deflation`.\n",
    "* We used a different suffix `.nc4` mainly not to overwrite the original file \n",
    "so we could compare their sizes. But it is not required for a deflated file - \n",
    "we could have used `.nc`. However, the `.nc4` suffix is used as an indication that\n",
    "a file is compressed.\n",
    "* `complevel` is the compression level. Choosing `complevel=1` ensures that no data is lost when it is deflated. \n",
    "A larger number will result in smaller files but <u>lossy</u> compression. \n",
    "This can be a problem with positive-definite variables that have a lot of zeros,\n",
    "like precipitaiton or shortwave radiation. Higher values of `complevel` can turn \n",
    "zeros into small negative or positive numbers that may cause problems with your \n",
    "calculations.\n",
    "* It is possible to apply `encoding` only to a subset of the variables in the Dataset. This is one way to \n",
    "selectively avoid problems like those above.\n",
    "* Note the `dict` function. This is an alternative way to write a python dictionary. \n",
    "`dict(zlib=True, complevel=1)` is the same as `{'zlib':True, 'complevel':1}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
