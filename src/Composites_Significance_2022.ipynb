{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating and Plotting a Composite With Significance\n",
    "\n",
    "What is a composite?  mean of a field conditioned on the value of another field.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "We have heard that ENSO changes atmospheric circulation and impacts precipitation and temperature globally.  We can explore this using composites. \n",
    "\n",
    "_Specific statement of the problem:_ \n",
    "\n",
    "What are the precipitation anomalies during El Nino vs. Neutral vs. La Nina _and how are they different_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSO\n",
    "\n",
    "We have an index called Nino3.4 that quantifies ENSO.  When it is large and positive, we say there is an El Nino.  When it is large and negative, we say there is a La Nina.  In between, we say it is neutral.\n",
    "\n",
    "Nino3.4 is calculated at the SST anomalies in a particular region in the Tropical Pacific.  We calculated this previously using the NOAA OISST data and then Subsetting, Groupby, and Aggregating. I wrote this data to a file:\n",
    "\n",
    "`/scratch/kpegion/nino34_1982-2019.oisstv2_anoms.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nino34='/home/pdirmeye/classes/clim680_2022/nino34_1982-2019.oisstv2_anoms.nc'\n",
    "ds_nino34=xr.open_dataset(file_nino34)\n",
    "ds_nino34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data goes from 1982-2019\n",
    "\n",
    "We can plot it as a timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining El Nino, La Nina, and Neutral\n",
    "\n",
    "Nino3.4 >= 1 -> El Nino\n",
    "\n",
    "Nino3.4 <= -1 -> La Nina\n",
    "\n",
    "Nino3.4 > -1 and Nino3.4 < 1 -> Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnino=ds_nino34['sst'].where(ds_nino34['sst']>=1)\n",
    "lanina=ds_nino34['sst'].where(ds_nino34['sst']<=-1)\n",
    "neutral=ds_nino34['sst'].where(np.logical_and(ds_nino34['sst']>-1,\n",
    "                                              ds_nino34['sst']<1))\n",
    "\n",
    "print('El Nino: ',elnino.count(dim='time').values)\n",
    "print('Neutral: ',neutral.count(dim='time').values)\n",
    "print('La Nina: ',lanina.count(dim='time').values)\n",
    "\n",
    "counts=[elnino.count(dim='time').values,\n",
    "        lanina.count(dim='time').values,\n",
    "        neutral.count(dim='time').values]\n",
    "print(counts)\n",
    "\n",
    "plt.plot(ds_nino34['time'],ds_nino34['sst'],'k')\n",
    "plt.fill_between(ds_nino34['time'].values,ds_nino34['sst'],color='lightgreen')\n",
    "plt.fill_between(ds_nino34['time'].values,elnino,y2=1.0,color='red')\n",
    "plt.fill_between(ds_nino34['time'].values,lanina,y2=-1.0,color='blue')\n",
    "plt.axhline(0,color='black',linewidth=0.5)\n",
    "plt.axhline(1,color='black',linewidth=0.5,linestyle='dotted')\n",
    "plt.axhline(-1,color='black',linewidth=0.5,linestyle='dotted') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation Data\n",
    "\n",
    "We will use the Global Precipitation Climatology Project (GPCP) Monthly Precipitation Data located on the COLA computers:\n",
    "\n",
    "https://kpegion.github.io/COLA-DATASETS-CATALOG/gpcp_precip.mon.mean.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/home/pdirmeye/classes/clim680_2022/GPCP_precip.mon.mean.nc'\n",
    "ds_precip = xr.open_dataset(file)\n",
    "ds_precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data goes from 1979-2020.  Let's select the same times as the nino34 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_precip = ds_precip.precip.sel(time=slice(ds_nino34['time'][0],ds_nino34['time'][-1]))\n",
    "\n",
    "da_climo = da_precip.groupby('time.month').mean()\n",
    "da_anoms = da_precip.groupby('time.month')-da_climo\n",
    "da_anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can select the dates that match El Nino, La Nina, and Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnino_precip = da_anoms.sel(time=elnino.dropna(dim='time')['time']).mean(dim='time')\n",
    "lanina_precip = da_anoms.sel(time=lanina.dropna(dim='time')['time']).mean(dim='time')\n",
    "neutral_precip = da_anoms.sel(time=neutral.dropna(dim='time')['time']).mean(dim='time')\n",
    "\n",
    "comp_precip = [elnino_precip,lanina_precip,neutral_precip]\n",
    "comp_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['El Nino','La Nina', 'Neutral']\n",
    "clevs = np.arange(-2.0,2.1,0.25)\n",
    "\n",
    "# Define the figure and each axis for the 3 rows and 3 columns\n",
    "fig, axs = plt.subplots(nrows=3,ncols=1,\n",
    "                        subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "                        figsize=(8.5,11))\n",
    "\n",
    "# axs is a 2 dimensional array of `GeoAxes`.  \n",
    "# We will flatten it into a 1-D array (just 3 rows)\n",
    "axs = axs.flatten()\n",
    "\n",
    "#Loop over all of the seasons and plot\n",
    "for i,enso in enumerate(comp_precip):\n",
    "\n",
    "        # Select the season\n",
    "        data = comp_precip[i]\n",
    "\n",
    "        # Add the cyclic point\n",
    "        data,lons = add_cyclic_point(data,coord=comp_precip[i]['lon'])\n",
    "\n",
    "        # Contour plot\n",
    "        cs=axs[i].contourf(lons,comp_precip[i]['lat'],data,clevs,\n",
    "                          transform = ccrs.PlateCarree(),\n",
    "                          cmap='BrBG',extend='both')\n",
    "\n",
    "       # Longitude labels\n",
    "        axs[i].set_xticks(np.arange(-180,181,60), crs=ccrs.PlateCarree())\n",
    "        lon_formatter = cticker.LongitudeFormatter()\n",
    "        axs[i].xaxis.set_major_formatter(lon_formatter)\n",
    "\n",
    "        # Latitude labels\n",
    "        axs[i].set_yticks(np.arange(-90,91,30), crs=ccrs.PlateCarree())\n",
    "        lat_formatter = cticker.LatitudeFormatter()\n",
    "        axs[i].yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "        \n",
    "        # Title each subplot with the name of the season\n",
    "        axs[i].set_title(labels[i]+' ('+str(counts[i])+')')\n",
    "\n",
    "        # Draw the coastines for each subplot\n",
    "        axs[i].coastlines()\n",
    "        \n",
    "# Adjust the location of the subplots \n",
    "# on the page to make room for the colorbar\n",
    "fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.95,\n",
    "                    wspace=0.1, hspace=0.5)\n",
    "\n",
    "# Add a colorbar axis at the bottom of the graph\n",
    "cbar_ax = fig.add_axes([0.25, 0.18, 0.5, 0.012])\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar = fig.colorbar(cs,cax=cbar_ax,orientation='horizontal',label='mm/day')\n",
    "\n",
    "# Add a big title at the top\n",
    "plt.suptitle('Composite Precipitation Anomalies during ENSO') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "--------------\n",
    "## Significance Testing\n",
    "\n",
    "We want to know where the El Niño or La Niña composites are truly different from the Neutral composite.  \n",
    "\n",
    "Last week we discussed the issue of trying to compare our neutral composite with our El Niño or La Niña composite, but we have issues of sample size.  \n",
    "\n",
    "* Neutral has 330 values\n",
    "* El Niño has 61 values\n",
    "* La Niña has 65 values\n",
    "\n",
    "Many more values went into calculating the mean for neutral. \n",
    "\n",
    "One solution to address this is to calculate statistical significance.\n",
    "\n",
    "### First, let's calculate differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_precip=[elnino_precip-neutral_precip, \n",
    "             lanina_precip-neutral_precip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot our difference maps\n",
    "\n",
    "Make the difference plots first and think about what they tell you scientifically before calculating and adding significance to them. \n",
    "\n",
    "Significance is a way of quantifying whether your results occurred by random chance.  \n",
    "\n",
    "If you don't have a scientific result, then significance will not create one.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['El Nino-Neutral','La Nina-Neutral']\n",
    "clevs=np.arange(-3,3.5,0.5)\n",
    "\n",
    "# Define the figure and each axis for the 3 rows and 3 columns\n",
    "fig, axs = plt.subplots(nrows=2,ncols=1,\n",
    "                        subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "                        figsize=(8.5,11))\n",
    "\n",
    "# axs is a 2 dimensional array of `GeoAxes`.  \n",
    "# We will flatten it into a 1-D array\n",
    "axs = axs.flatten()\n",
    "\n",
    "#Loop over all of the seasons and plot\n",
    "for i,enso in enumerate(diff_precip):\n",
    "\n",
    "        # Select the season\n",
    "        data = diff_precip[i]\n",
    "\n",
    "        # Add the cyclic point\n",
    "        data,lons = add_cyclic_point(data,coord=diff_precip[i]['lon'])\n",
    "\n",
    "        # Contour plot\n",
    "        cs = axs[i].contourf(lons,diff_precip[i]['lat'],data,clevs,\n",
    "                          transform=ccrs.PlateCarree(),\n",
    "                          cmap='BrBG',extend='both')\n",
    "\n",
    "       # Longitude labels\n",
    "        axs[i].set_xticks(np.arange(-180,181,60), crs=ccrs.PlateCarree())\n",
    "        lon_formatter = cticker.LongitudeFormatter()\n",
    "        axs[i].xaxis.set_major_formatter(lon_formatter)\n",
    "\n",
    "        # Latitude labels\n",
    "        axs[i].set_yticks(np.arange(-90,91,30), crs=ccrs.PlateCarree())\n",
    "        lat_formatter = cticker.LatitudeFormatter()\n",
    "        axs[i].yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "        \n",
    "        # Title each subplot with the name of the season\n",
    "        axs[i].set_title(labels[i])\n",
    "\n",
    "        # Draw the coastines for each subplot\n",
    "        axs[i].coastlines()\n",
    "        \n",
    "# Adjust the location of the subplots \n",
    "# on the page to make room for the colorbar\n",
    "fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.95,\n",
    "                    wspace=0.1, hspace=0.5)\n",
    "\n",
    "# Add a colorbar axis at the bottom of the graph\n",
    "cbar_ax = fig.add_axes([0.25, 0.18, 0.5, 0.012])\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar = fig.colorbar(cs, cax=cbar_ax,orientation='horizontal',label='mm/day')\n",
    "\n",
    "# Add a big title at the top\n",
    "plt.suptitle('Composite Precipitation Differences during ENSO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Significance Using the built-in Stats Functions\n",
    "\n",
    "Python has a library of functions called `scipy`, which stands for \"Science Python\". \n",
    "It includes many packages and functions of use in science, which are more _heavy duty_ than what most\n",
    "non-scientist users need. \n",
    "One of those packages is `scipy.stats`, which includes many common (and uncommon) statistical functions.\n",
    "\n",
    "A major element of Climate Science is testing whether a result is _statistically significant_, i.e., \n",
    "significantly different that what might occur due to expected natural variations; different from _chance_.\n",
    "You will have a difficult time convincing anyone that your research conclusions are robust and meaningful\n",
    "unless you demonstrate they are _statistically significant_.\n",
    "\n",
    "To do this correctly, you need to think carefully about your analysis and define precisely your \n",
    "problem and the meaning of its possible outcomes. \n",
    "This is typically done by defining a **null hypothesis** that defines a lack of significance, and then\n",
    "testing whether it can be disproved.\n",
    "\n",
    "__State our problem clearly__\n",
    "\n",
    "Where is the mean difference different enough to be considered significant (meaning its unlikely it happened by random chance)?\n",
    "\n",
    "$\\mu_x$ = El Nino Composite ($n_x$=61)\n",
    "\n",
    "$\\mu_y$ = Neutral Composite ($n_y$=330)\n",
    "\n",
    "__Define our Null Hypothesis__\n",
    "\n",
    "Null Hypothesis (H0): Differences between the two means is zero\n",
    "\n",
    "$\\mu_x-\\mu_y=0$\n",
    "\n",
    "__Pick a level of significance__\n",
    "\n",
    "Pick a p-value (or level of significance).  Typically, 1% or 5% is chosen.\n",
    "\n",
    "You must pick this level first. \n",
    "\n",
    "* It is cheating to pick it after you see the results. \n",
    "* It is also cheating to make changes to your analysis to get a desirable p-value.\n",
    "\n",
    "Doing things to your data or analysis to get a higher p-value is \n",
    "called [P-hacking](https://www.nature.com/news/scientific-method-statistical-errors-1.14700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.05 # 95% confidence level, or 5% likelihood that the result arose by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the stats `ttest_ind` function\n",
    "\n",
    "Let's follow the documentation to understand what this function does...\n",
    "\n",
    "[ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)\n",
    "\n",
    "This is a two-sided test, based on the __Student's t-Test__, for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.\n",
    "\n",
    "#### Input: \n",
    "\n",
    "**Two arrays (a,b) to compare the means and perform a ttest**\n",
    "* a = values that go into elnino_precip composite \n",
    "* b = values that go into neutral_precip\n",
    "\n",
    "We can set several other optional parameters for this function:\n",
    "\n",
    "**axis:  Axis along which to compute test. If None, compute over the whole arrays, a, and b.**\n",
    "\n",
    "* We want this calculated over our whole grid/array, so no need to specify\n",
    "\n",
    "**equal_var: If True (default), perform a standard independent 2 sample test that assumes equal population variances [1]. If False, perform Welch’s t-test, which does not assume equal population variance [2].**\n",
    "\n",
    "* We probably don't have equal variance, so set this to `False`\n",
    "\n",
    "**nan_policy: {‘propagate’, ‘raise’, ‘omit’}, optional\n",
    "Defines how to handle when input contains nan. The following options are available (default is ‘propagate’):**\n",
    "\n",
    "**`propagate`: returns nan**\n",
    "\n",
    "**`raise`: throws an error**\n",
    "\n",
    "**`omit`: performs the calculations ignoring nan values**\n",
    "\n",
    "* We can leave this as default\n",
    "\n",
    "#### Output \n",
    "\n",
    "**`t-statistic`: The calculated t-statistic.**\n",
    "\n",
    "**`pvalue`: The two-tailed p-value. This is the p-value telling us the % chance we have in being incorrect if we reject the null hypothesis. This is the information we want. We will identify where this `pvalue` is less than our `p=0.05`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "elnino_precip_vals = da_anoms.sel(time=elnino.dropna(dim='time')['time'])\n",
    "lanina_precip_vals = da_anoms.sel(time=lanina.dropna(dim='time')['time'])\n",
    "neutral_precip_vals = da_anoms.sel(time=neutral.dropna(dim='time')['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat,pval = ttest_ind(elnino_precip_vals, \n",
    "                neutral_precip_vals, \n",
    "                equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(pval,[0,0.05,1],cmap='RdPu_r') \n",
    "plt.colorbar(orientation='horizontal') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a mask of where the value is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_diff = diff_precip[0].where(pval <= p)\n",
    "mask_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our difference between composites and also mark where its significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['El Nino vs. Neutral']\n",
    "clevs=np.arange(-3,3.5,0.5)\n",
    "hatch_levs=[0]\n",
    "\n",
    "# Define the figure and each axis for the 2 rows and 1 columns\n",
    "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
    "                        subplot_kw={'projection': ccrs.PlateCarree(central_longitude=200.0)})\n",
    "\n",
    "\n",
    "data=diff_precip[0]\n",
    "\n",
    "# Add the cyclic point\n",
    "data,lons=add_cyclic_point(data,coord=diff_precip[0]['lon'])\n",
    "mask_data,lons=add_cyclic_point(mask_diff,coord=diff_precip[0]['lon'])\n",
    "\n",
    "# Contour plot\n",
    "cs=axs.contourf(lons,diff_precip[0]['lat'],data,clevs,\n",
    "                transform = ccrs.PlateCarree(),\n",
    "                cmap='BrBG',extend='both')\n",
    "\n",
    "# Overlay of stippling\n",
    "ss=axs.contourf(lons,diff_precip[0]['lat'],mask_data,[0,1],\n",
    "                transform = ccrs.PlateCarree(),colors=['None'],\n",
    "                hatches=['.',''],extend='both',alpha=0.0)\n",
    "\n",
    "# Draw the coastines for each subplot\n",
    "axs.coastlines()\n",
    "        \n",
    "# Adjust the location of the subplots \n",
    "# on the page to make room for the colorbar\n",
    "fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.95,\n",
    "                    wspace=0.1, hspace=0.5)\n",
    "\n",
    "# Add a colorbar axis at the bottom of the graph\n",
    "cbar_ax = fig.add_axes([0.15, 0.18, 0.7, 0.025])\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar = fig.colorbar(cs,cax=cbar_ax,orientation='horizontal',label='mm/day')\n",
    "\n",
    "# Add a big title at the top\n",
    "plt.suptitle('Composite Precipitation Differences El Nino-Neutral')\n",
    "\n",
    "plt.savefig('stats_ttest.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Way to Present Significance Data\n",
    "\n",
    "Using semi-transparent shading to mask insignficant areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_diff_o = xr.ones_like(diff_precip[0]).where(pval > p) # This retains data where the pval is insignificant\n",
    "\n",
    "\n",
    "labels=['El Nino vs. Neutral']\n",
    "clevs=np.arange(-3,3.5,0.5)\n",
    "hatch_levs=[0]\n",
    "\n",
    "# Define the figure and each axis for the 2 rows and 1 columns\n",
    "fig, axs = plt.subplots(nrows=1,ncols=1,\n",
    "                        subplot_kw={'projection': ccrs.PlateCarree(central_longitude=200.0)})\n",
    "\n",
    "data=diff_precip[0]\n",
    "\n",
    "# Add the cyclic point\n",
    "data,lons=add_cyclic_point(data,coord=diff_precip[0]['lon'])\n",
    "mask_data_o,lons=add_cyclic_point(mask_diff_o,coord=diff_precip[0]['lon'])\n",
    "\n",
    "# Contour plot\n",
    "cs=axs.contourf(lons,diff_precip[0]['lat'],data,clevs,\n",
    "                transform = ccrs.PlateCarree(),\n",
    "                cmap='BrBG',extend='both')\n",
    "\n",
    "# Mask out insignficant areas with semi-transparent shading\n",
    "ss=axs.contourf(lons,diff_precip[0]['lat'],mask_data_o,\n",
    "                transform = ccrs.PlateCarree(),colors=['k'],\n",
    "                alpha=0.3)\n",
    "\n",
    "# Draw the coastines for each subplot\n",
    "axs.coastlines()\n",
    "        \n",
    "# Adjust the location of the subplots \n",
    "# on the page to make room for the colorbar\n",
    "fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.95,\n",
    "                    wspace=0.1, hspace=0.5)\n",
    "\n",
    "# Add a colorbar axis at the bottom of the graph\n",
    "cbar_ax = fig.add_axes([0.15, 0.18, 0.7, 0.025])\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar = fig.colorbar(cs,cax=cbar_ax,orientation='horizontal',label='mm/day')\n",
    "\n",
    "# Add a big title at the top\n",
    "plt.suptitle('Composite Precipitation Differences El Nino-Neutral')\n",
    "\n",
    "plt.savefig('stats_ttest.png',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "### Non-parametric significance testing\n",
    "The use of statistics like standard deviation to estimate significance is common practice, but it is based on assumptions.\n",
    "Specifically, that the data being examined has the following properties:\n",
    "1. The data have a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "2. The important relationships between data sets are [linear](https://en.wikipedia.org/wiki/Linearity)\n",
    "\n",
    "We will talk more about linearity later, when we discuss regressions and correlation. But the Student's t-Test we used above makes the assumption that anomalies in the data are normally distributed.  This is often a good assumption for many climate variables (e.g., temperature). However, it is not so good for other, e.g., precipitation, especially when the data are sampled on a short time interval (e.g., daily). What about the monthly data examined here?  Well, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "#lat,lon = 39,-79\n",
    "lat,lon = -30,140\n",
    "grid_anoms = da_anoms.sel(lat=lat,lon=lon,method='nearest')\n",
    "\n",
    "spacing = 0.2\n",
    "xvals = np.arange(-4,4.1,spacing)\n",
    "p_distrib = plt.hist(grid_anoms,bins=xvals,label=\"Observed\")\n",
    "\n",
    "plt.plot(xvals,spacing*grid_anoms.count().data*norm.pdf(xvals,grid_anoms.mean(),grid_anoms.std()),label=\"Normal Dist.\")\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Count [months]\")\n",
    "plt.xlabel(\"Anomalies [mm/day]\")\n",
    "\n",
    "plt.axvline(0,color='w',linewidth=0.5)\n",
    "\n",
    "slon = f\"{lon}˚E\"\n",
    "if lon < 0: slon=f\"{-lon}˚W\"\n",
    "slat = f\"{lat}˚N\"\n",
    "if lat < 0: slat=f\"{-lat}˚S\"\n",
    "\n",
    "plt.title(f\"Distribution of Monthly Precipitation Anomalies at {slat} {slon}\") ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the precipitation distribution is [skewed](https://en.wikipedia.org/wiki/Skewness) positive (i.e., the long tail is on the positive side of the distribution and the median is less than the mean). This is common for precipitation data.\n",
    "\n",
    "What can we do?\n",
    "\n",
    "### Nonparametric statistics\n",
    "\n",
    "Rather than assume a distribution for the data, we can query the data directly to find where an anomaly ranks among all the samples in the dataset. This is a form of [nonparameteric statistics](https://en.wikipedia.org/wiki/Nonparametric_statistics) where no assumptions are made. This also makes conclusions more robust. With today's computing power, it is often fairly easy to test the significance of an anomaly by simply comparing its magnitude to available past data.\n",
    "\n",
    "For this location, let's find the likelihood of a monthly precipitation anomaly greater than 2 mm/day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val = 2.0\n",
    "\n",
    "# Based on the assumption of a normal distribution:\n",
    "norm_prob = norm.sf(test_val,grid_anoms.mean(),grid_anoms.std())\n",
    "\n",
    "# Based on the actual data distribution:\n",
    "epsilon = 1e-6\n",
    "test_bin = (np.argwhere(abs(p_distrib[1] - test_val) < epsilon)[0][0]) # Find histogram bin corresponding to the the test value\n",
    "data_prob = 1.0 - p_distrib[0][:test_bin].sum() / grid_anoms.count().data\n",
    "\n",
    "print(f\"Probability of >2 mm/day monthly anomaly at {slat} {slon}: \\\n",
    "       \\nNormal data distribution = {100*norm_prob:0.2f}% \\\n",
    "       \\nActual data distribution = {100*data_prob:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can flip the question around and ask at what anomaly value would we expect only a 1% chance of \n",
    "precipitation rates exceeding that value for the month (i.e., 99% chance precipitation rates are below the value). \n",
    "This uses the `quantile` method in `numpy`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance = 0.99\n",
    "\n",
    "# Based on the assumption of a normal distribution:\n",
    "norm_value = norm.ppf(chance,grid_anoms.mean(),grid_anoms.std())\n",
    "\n",
    "# Based on the actual data distribution:\n",
    "data_value = np.quantile(grid_anoms,chance)\n",
    "\n",
    "print(f\"Rainfall rate corresponding to the {100*chance:0.0f}th percentile at {slat} {slon}: \\\n",
    "       \\nNormal data distribution = {norm_value:0.2f} mm/day \\\n",
    "       \\nActual data distribution = {data_value:0.2f} mm/day\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_data",
   "language": "python",
   "name": "clim_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
